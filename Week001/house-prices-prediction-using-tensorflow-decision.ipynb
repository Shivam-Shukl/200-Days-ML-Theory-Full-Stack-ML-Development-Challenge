{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# House Prices Prediction using TensorFlow Decision Forests","metadata":{}},{"cell_type":"markdown","source":"## Import the Library","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:31:55.245099Z","iopub.execute_input":"2025-05-25T05:31:55.245345Z","iopub.status.idle":"2025-05-25T05:32:09.924198Z","shell.execute_reply.started":"2025-05-25T05:31:55.245324Z","shell.execute_reply":"2025-05-25T05:32:09.923462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Tensorflow v\"+ tf.__version__)\nprint(\"Tensorflow decision v\" + tfdf.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:09.925407Z","iopub.execute_input":"2025-05-25T05:32:09.925879Z","iopub.status.idle":"2025-05-25T05:32:09.929425Z","shell.execute_reply.started":"2025-05-25T05:32:09.925861Z","shell.execute_reply":"2025-05-25T05:32:09.929009Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading Dataset","metadata":{}},{"cell_type":"code","source":"train_file_path = \"../input/house-prices-advanced-regression-techniques/train.csv\"\ndataset_df = pd.read_csv(train_file_path)\nprint(\"Full train dataset shape is {}\".format(dataset_df.shape))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:09.930311Z","iopub.execute_input":"2025-05-25T05:32:09.930610Z","iopub.status.idle":"2025-05-25T05:32:10.004502Z","shell.execute_reply.started":"2025-05-25T05:32:09.930593Z","shell.execute_reply":"2025-05-25T05:32:10.003928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data has 81 columns with 1460 entries.We can get to know by printing top 5 entries.","metadata":{}},{"cell_type":"code","source":"dataset_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:10.005136Z","iopub.execute_input":"2025-05-25T05:32:10.005330Z","iopub.status.idle":"2025-05-25T05:32:10.032593Z","shell.execute_reply.started":"2025-05-25T05:32:10.005313Z","shell.execute_reply":"2025-05-25T05:32:10.031964Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are 79 features columns.Using thes features model has to predict the house sales price indicated by the label column named SalePrice.\n\nWe will drop the ID column as it is not necessary for model training.","metadata":{}},{"cell_type":"code","source":"dataset_df = dataset_df.drop('Id', axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:10.034223Z","iopub.execute_input":"2025-05-25T05:32:10.034408Z","iopub.status.idle":"2025-05-25T05:32:10.040149Z","shell.execute_reply.started":"2025-05-25T05:32:10.034394Z","shell.execute_reply":"2025-05-25T05:32:10.039550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:10.040727Z","iopub.execute_input":"2025-05-25T05:32:10.040884Z","iopub.status.idle":"2025-05-25T05:32:10.061438Z","shell.execute_reply.started":"2025-05-25T05:32:10.040867Z","shell.execute_reply":"2025-05-25T05:32:10.060772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Getting through the features.","metadata":{}},{"cell_type":"code","source":"dataset_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:10.062231Z","iopub.execute_input":"2025-05-25T05:32:10.062483Z","iopub.status.idle":"2025-05-25T05:32:10.089053Z","shell.execute_reply.started":"2025-05-25T05:32:10.062460Z","shell.execute_reply":"2025-05-25T05:32:10.088322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataset_df['SalePrice'].describe())\nplt.figure(figsize=(9, 8))\nsns.distplot(dataset_df['SalePrice'], color='g', bins=100, hist_kws={'alpha': 0.4});","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:10.089687Z","iopub.execute_input":"2025-05-25T05:32:10.089878Z","iopub.status.idle":"2025-05-25T05:32:10.403321Z","shell.execute_reply.started":"2025-05-25T05:32:10.089862Z","shell.execute_reply":"2025-05-25T05:32:10.402631Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Numerical data distribution\n\nWe will look at how the numerical features are distributed. In order to do this, let us first list all the types of data from our dataset and select only the numerical ones.","metadata":{}},{"cell_type":"code","source":"list(set(dataset_df.dtypes.tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:10.404173Z","iopub.execute_input":"2025-05-25T05:32:10.404317Z","iopub.status.idle":"2025-05-25T05:32:10.409184Z","shell.execute_reply.started":"2025-05-25T05:32:10.404304Z","shell.execute_reply":"2025-05-25T05:32:10.408630Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now let us plot the distribution for all the numerical features.","metadata":{}},{"cell_type":"code","source":"df_num = dataset_df.select_dtypes(include = ['float64', 'int64'])\ndf_num.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:10.409705Z","iopub.execute_input":"2025-05-25T05:32:10.409893Z","iopub.status.idle":"2025-05-25T05:32:10.430846Z","shell.execute_reply.started":"2025-05-25T05:32:10.409876Z","shell.execute_reply":"2025-05-25T05:32:10.430140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_num.hist(figsize =(16,20),bins = 50 ,xlabelsize =8,ylabelsize =8);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:10.431592Z","iopub.execute_input":"2025-05-25T05:32:10.431845Z","iopub.status.idle":"2025-05-25T05:32:15.895338Z","shell.execute_reply.started":"2025-05-25T05:32:10.431829Z","shell.execute_reply":"2025-05-25T05:32:15.894575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare the Dataset\n\nThis dataset includes a combination of numeric, categorical, and missing features. Fortunately, TensorFlow Decision Forests (TF-DF) can handle all these feature types directly, without the need for manual preprocessing. This built-in flexibility makes tree-based models an excellent starting point for learning machine learning with TensorFlow.\n\nNext, let's split the dataset into training and testing sets.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef split_dataset(dataset, test_ratio=0.30):\n  test_indices = np.random.rand(len(dataset)) < test_ratio\n  return dataset[~test_indices], dataset[test_indices]\n\ntrain_ds_pd, valid_ds_pd = split_dataset(dataset_df)\nprint(\"{} examples in training, {} examples in testing.\".format(\n    len(train_ds_pd), len(valid_ds_pd)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:15.896105Z","iopub.execute_input":"2025-05-25T05:32:15.896376Z","iopub.status.idle":"2025-05-25T05:32:15.903684Z","shell.execute_reply.started":"2025-05-25T05:32:15.896353Z","shell.execute_reply":"2025-05-25T05:32:15.903078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Before training the model, there's one more important step: converting the dataset from a Pandas DataFrame (pd.DataFrame) to a TensorFlow Dataset (tf.data.Dataset).\n\nTensorFlow Datasets provide efficient data pipelines, which are especially useful when training models on hardware accelerators like GPUs or TPUs.\n\nAlso, since the default setting for the Random Forest model is classification, and our task is regression, we need to explicitly specify the task type using tfdf.keras.Task.REGRESSION.","metadata":{}},{"cell_type":"code","source":"label = 'SalePrice'\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label, task = tfdf.keras.Task.REGRESSION)\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=label, task = tfdf.keras.Task.REGRESSION)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:15.904290Z","iopub.execute_input":"2025-05-25T05:32:15.904515Z","iopub.status.idle":"2025-05-25T05:32:16.114512Z","shell.execute_reply.started":"2025-05-25T05:32:15.904496Z","shell.execute_reply":"2025-05-25T05:32:16.113808Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Select a Model\nTensorFlow Decision Forests offers multiple tree-based models to choose from:\n\n- RandomForestModel\n\n- GradientBoostedTreesModel\n\n- CartModel\n\n- DistributedGradientBoostedTreesModel\n\nTo begin with, we'll use the Random Forest model â€” one of the most popular and widely used decision forest algorithms.\n\nA Random Forest is an ensemble of decision trees, where each tree is trained independently on a randomly sampled subset of the training data (with replacement). This approach makes the model robust to overfitting and easy to use, even with minimal hyperparameter tuning.\n\nYou can view all the available models in TensorFlow Decision Forests using the following command:","metadata":{}},{"cell_type":"code","source":"tfdf.keras.get_all_models()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:16.116457Z","iopub.execute_input":"2025-05-25T05:32:16.116715Z","iopub.status.idle":"2025-05-25T05:32:16.121320Z","shell.execute_reply.started":"2025-05-25T05:32:16.116699Z","shell.execute_reply":"2025-05-25T05:32:16.120711Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## How to Configure the Models\n\nTensorFlow Decision Forests comes with well-optimized default settings, including top-performing hyperparameters based on internal benchmarksâ€”tweaked to ensure efficient training time.\n\nHowever, if you want to fine-tune the model for better accuracy, you have the flexibility to customize various hyperparameters.\n\nYou can start by selecting a predefined hyperparameter template and specifying key parameters like this:","metadata":{}},{"cell_type":"markdown","source":"rf = tfdf.keras.RandomForestModel(hyperparameter_template=\"benchmark_rank1\", task=tfdf.keras.Task.REGRESSION)","metadata":{}},{"cell_type":"markdown","source":"## Create a Random Forest\nFor todayâ€™s implementation, weâ€™ll use the default settings to create a Random Forest model, while specifying the task type as tfdf.keras.Task.REGRESSION to indicate that this is a regression problem.","metadata":{}},{"cell_type":"code","source":"rf = tfdf.keras.RandomForestModel(task = tfdf.keras.Task.REGRESSION)\nrf.compile(metrics=[\"mse\"]) # Optional, you can use this to include a list of eval metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:16.121952Z","iopub.execute_input":"2025-05-25T05:32:16.122123Z","iopub.status.idle":"2025-05-25T05:32:16.186710Z","shell.execute_reply.started":"2025-05-25T05:32:16.122107Z","shell.execute_reply":"2025-05-25T05:32:16.186064Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train the Model\nTraining the model can be done with a simple one-liner.","metadata":{}},{"cell_type":"code","source":"rf.fit(x=train_ds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:32:16.187369Z","iopub.execute_input":"2025-05-25T05:32:16.187545Z","iopub.status.idle":"2025-05-25T05:32:22.931926Z","shell.execute_reply.started":"2025-05-25T05:32:16.187530Z","shell.execute_reply":"2025-05-25T05:32:22.931318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize the Model\nOne advantage of tree-based models is their interpretabilityâ€”you can easily visualize individual decision trees. By default, the Random Forest model contains 300 trees. Below, weâ€™ll display one of them (specifically, the first tree) up to a maximum depth of 3:","metadata":{}},{"cell_type":"code","source":"tree = rf.make_inspector().extract_tree(tree_idx=0)\nprint(tree)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:47:07.487212Z","iopub.execute_input":"2025-05-25T05:47:07.487488Z","iopub.status.idle":"2025-05-25T05:47:07.520619Z","shell.execute_reply.started":"2025-05-25T05:47:07.487472Z","shell.execute_reply":"2025-05-25T05:47:07.520024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n##  Evaluate the Model Using OOB Data and Validation Dataset\n\n* Before training, we manually set aside **20% of the dataset as a validation set**, named `valid_ds`.\n* In addition to this, we can evaluate the **Random Forest model** using the **Out-of-Bag (OOB) score**.\n\n---\n\n###  What is OOB Data?\n\n* During training, the Random Forest algorithm samples random subsets of the training data **with replacement**.\n* The **samples not selected** in a particular tree are referred to as **Out-of-Bag (OOB)** data.\n* These OOB samples act like an internal validation set used to estimate model performance.\n* The model computes an **OOB score** using this data, helping to assess its generalization without needing a separate validation set.\n\n [Learn more about OOB data here.](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr)\n\n---\n\n###  Interpreting the RMSE Plot\n\n* The training log shows how the **Root Mean Squared Error (RMSE)** on OOB data evolves as more trees are added.\n* We can plot this to visualize model performance.\n* **Note:** For RMSE, **lower values indicate better performance**.\n\n\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nlogs = rf.make_inspector().training_logs()\nplt.plot([log.num_trees for log in logs], [log.evaluation.rmse for log in logs])\nplt.xlabel(\"Number of trees\")\nplt.ylabel(\"RMSE (out-of-bag)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:49:58.322299Z","iopub.execute_input":"2025-05-25T05:49:58.322574Z","iopub.status.idle":"2025-05-25T05:49:58.443282Z","shell.execute_reply.started":"2025-05-25T05:49:58.322555Z","shell.execute_reply":"2025-05-25T05:49:58.442693Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also see some general stats on the OOB dataset:","metadata":{}},{"cell_type":"code","source":"inspector = rf.make_inspector()\ninspector.evaluation()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:50:12.880944Z","iopub.execute_input":"2025-05-25T05:50:12.881185Z","iopub.status.idle":"2025-05-25T05:50:12.899265Z","shell.execute_reply.started":"2025-05-25T05:50:12.881167Z","shell.execute_reply":"2025-05-25T05:50:12.898556Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, let us run an evaluation using the validation dataset.","metadata":{}},{"cell_type":"code","source":"evaluation = rf.evaluate(x=valid_ds,return_dict=True)\n\nfor name, value in evaluation.items():\n  print(f\"{name}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:51:01.627266Z","iopub.execute_input":"2025-05-25T05:51:01.627511Z","iopub.status.idle":"2025-05-25T05:51:02.245209Z","shell.execute_reply.started":"2025-05-25T05:51:01.627496Z","shell.execute_reply":"2025-05-25T05:51:02.244554Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n##  Variable Importances\n\nVariable importance helps us understand how much each feature contributes to the model's predictions or performance.\n\nTensorFlow Decision Forests provides several methods to evaluate feature importance in Decision Tree models.\n\nLetâ€™s explore the different types of variable importance metrics available.\n","metadata":{}},{"cell_type":"code","source":"print(f\"Available variable importances:\")\nfor importance in inspector.variable_importances().keys():\n  print(\"\\t\", importance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:53:35.606021Z","iopub.execute_input":"2025-05-25T05:53:35.606259Z","iopub.status.idle":"2025-05-25T05:53:35.610802Z","shell.execute_reply.started":"2025-05-25T05:53:35.606243Z","shell.execute_reply":"2025-05-25T05:53:35.610044Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n###  Example: NUM\\_AS\\_ROOT Variable Importance\n\nLetâ€™s look at feature importance using the `NUM_AS_ROOT` metric.\n\n* A **higher score** means the feature is **more frequently used as the root node** across the trees in the forest.\n* Features at the **top of the list** have the **strongest influence** on model predictions.\n* The output is **sorted by importance**, with the most impactful features listed first.\n","metadata":{}},{"cell_type":"code","source":"inspector.variable_importances()[\"NUM_AS_ROOT\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:55:15.963339Z","iopub.execute_input":"2025-05-25T05:55:15.963591Z","iopub.status.idle":"2025-05-25T05:55:15.969063Z","shell.execute_reply.started":"2025-05-25T05:55:15.963575Z","shell.execute_reply":"2025-05-25T05:55:15.968406Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plot the variable importances from the inspector using Matplotlib","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\n\n# Variable importance metric: number of times features are used as root nodes.\nvariable_importance_metric = \"NUM_AS_ROOT\"\nvariable_importances = inspector.variable_importances()[variable_importance_metric]\n\n# Retrieve feature names and their corresponding importance scores.\n# `variable_importances` contains tuples of (feature, importance).\nfeature_names = [vi[0].name for vi in variable_importances]\nfeature_importances = [vi[1] for vi in variable_importances]\n\n# Features are sorted by decreasing importance.\nfeature_ranks = range(len(feature_names))\n\nbar = plt.barh(feature_ranks, feature_importances, label=[str(x) for x in feature_ranks])\nplt.yticks(feature_ranks, feature_names)\nplt.gca().invert_yaxis()\n\n# TODO: Update to use \"plt.bar_label()\" when it becomes available.\n# Annotate each bar with its importance value.\nfor importance, patch in zip(feature_importances, bar.patches):\n    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{importance:.4f}\", va=\"top\")\n\nplt.xlabel(variable_importance_metric)\nplt.title(\"NUM_AS_ROOT Importance for class 1 vs others\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:57:16.835349Z","iopub.execute_input":"2025-05-25T05:57:16.835590Z","iopub.status.idle":"2025-05-25T05:57:17.017325Z","shell.execute_reply.started":"2025-05-25T05:57:16.835573Z","shell.execute_reply":"2025-05-25T05:57:17.016509Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## Submission\n\nFinally, use the trained model to make predictions on the competitionâ€™s test dataset.\n","metadata":{}},{"cell_type":"code","source":"test_file_path = \"../input/house-prices-advanced-regression-techniques/test.csv\"\ntest_data = pd.read_csv(test_file_path)\nids = test_data.pop('Id')\n\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(\n    test_data,\n    task = tfdf.keras.Task.REGRESSION)\n\npreds = rf.predict(test_ds)\noutput = pd.DataFrame({'Id': ids,\n                       'SalePrice': preds.squeeze()})\n\noutput.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T05:59:49.241769Z","iopub.execute_input":"2025-05-25T05:59:49.242015Z","iopub.status.idle":"2025-05-25T05:59:50.069267Z","shell.execute_reply.started":"2025-05-25T05:59:49.241999Z","shell.execute_reply":"2025-05-25T05:59:50.068549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')\nsample_submission_df['SalePrice'] = rf.predict(test_ds)\nsample_submission_df.to_csv('/kaggle/working/submission.csv', index=False)\nsample_submission_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T06:00:24.174032Z","iopub.execute_input":"2025-05-25T06:00:24.174702Z","iopub.status.idle":"2025-05-25T06:00:24.402257Z","shell.execute_reply.started":"2025-05-25T06:00:24.174679Z","shell.execute_reply":"2025-05-25T06:00:24.401716Z"}},"outputs":[],"execution_count":null}]}